{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Pk4QXDZuhdBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd gdrive/MyDrive/deep_learning/computer_vision/convnet/"
      ],
      "metadata": {
        "id": "B5MqsL4jhc-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Statements\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import randint\n",
        "import utils\n",
        "import time"
      ],
      "metadata": {
        "id": "EXQu4UCRhc7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect GPU\n",
        "device= torch.device(\"cuda\")\n",
        "#device= torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "khqogLmmhc4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "metadata": {
        "id": "u2shVmKLhc1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import data\n",
        "train_data=torch.load('../../data/cifar/train_data.pt')\n",
        "train_label=torch.load('../../data/cifar/train_label.pt')\n",
        "test_data=torch.load('../../data/cifar/test_data.pt')\n",
        "test_label=torch.load('../../data/cifar/test_label.pt')\n",
        "\n",
        "print(train_data.size())\n",
        "print(test_data.size())"
      ],
      "metadata": {
        "id": "lLHoG4ulhczM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data augmentation and EDA\n",
        "train_data_inverse = torch.flip(train_data,dims=[3])\n",
        "train_data = torch.cat( (train_data, train_data_inverse),0, )\n",
        "train_label = torch.cat( (train_label, train_label),0,)\n",
        "\n",
        "print(train_data.size())\n",
        "\n",
        "mean = train_data.mean()\n",
        "std = train_data.std()\n",
        "\n",
        "print(mean)\n",
        "print(std)"
      ],
      "metadata": {
        "id": "K-kR8pXMhcwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sending data to the GPU\n",
        "train_data = train_data.to(device)\n",
        "train_label = train_label.to(device)\n",
        "test_data = test_data.to(device)\n",
        "test_label = test_label.to(device)"
      ],
      "metadata": {
        "id": "L7ltxaoyhct6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i4qZCFRhWr6"
      },
      "outputs": [],
      "source": [
        "#Basic block\n",
        "class BasicBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        #conv: bs x C x H x W  -->  bs x C x H x W\n",
        "        self.conv = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1)\n",
        "        #batch normalization\n",
        "        self.bn = nn.BatchNorm2d(input_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv(x)\n",
        "        y = F.relu(y)\n",
        "        y = self.bn(y)\n",
        "\n",
        "        z = self.conv(y)\n",
        "        z = F.relu(z)\n",
        "        z = self.bn(z)\n",
        "\n",
        "        output = x + z\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convnet Architecture\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_channels, classes, layers):\n",
        "        super().__init__()\n",
        "\n",
        "        #Initial convolutional layer:  bs x 3 x 32 x 32  --> bs x 16 x 32 x 32\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)   #C: 3-->16, H&W: retained\n",
        "        self.batchnorm = nn.BatchNorm2d(16)                                 #16 is the number of input channels\n",
        "\n",
        "        #Basic/Residual Blocks 1: bs x 16 x 32 x 32 --> bs x 16 x 32 x 32\n",
        "        basic_block_list1 = []\n",
        "        for i in range(layers):\n",
        "            basic_block_list1.append(BasicBlock(16))\n",
        "        self.block_list1 = nn.ModuleList(basic_block_list1)\n",
        "\n",
        "        #Bridge Block 1 bs x 16 x 32 x 32 --> bs x 32 x 16 x 16\n",
        "        #WITH BLOCK: self.bridge1 = BridgeBlock(input_size)\n",
        "        self.bridge1_conv1 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.bridge1_bn1 = nn.BatchNorm2d(32)\n",
        "        self.bridge1_conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bridge1_bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        #Basic/Residual Blocks 2: bs x 32 x 16 x 16 --> bs x 32 x 16 x 16\n",
        "        basic_block_list2 = []\n",
        "        for i in range(layers):\n",
        "            basic_block_list2.append(BasicBlock(32))\n",
        "        self.block_list2 = nn.ModuleList(basic_block_list2)\n",
        "\n",
        "        #Bridge Block 2 bs x 32 x 16 x 16 --> bs x 64 x 8 x 8\n",
        "        #self.bridge2 = BridgeBlock(hidden_size, 2*hidden_size)\n",
        "        self.bridge2_conv1 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.bridge2_bn1 = nn.BatchNorm2d(64)\n",
        "        self.bridge2_conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bridge2_bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        #Basic/Residual Blocks 3: bs x 64 x 8 x 8 --> bs x 64 x 8 x 8\n",
        "        basic_block_list3 = []\n",
        "        for i in range(layers):\n",
        "            basic_block_list3.append(BasicBlock(64))\n",
        "        self.block_list3 = nn.ModuleList(basic_block_list3)\n",
        "\n",
        "        #Final pooling: bs x 64 x 8 x 8 --> bs x 64 x 1 x 1\n",
        "        self.avg_pool = nn.AvgPool2d(8)     #Used to be 8\n",
        "        #Final layer: bs x 64 x 1 x 1 --> bs x 10\n",
        "        self.linear_layer = nn.Linear(64, classes , bias=False)    #Used to be 64\n",
        "\n",
        "        #Other\n",
        "        self.layers = layers\n",
        "        self.input_channels = input_channels\n",
        "        self.classes = classes\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #Initial convolutional layer\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm(x)\n",
        "\n",
        "        #Basic/Residual Blocks 1\n",
        "        for i in range(self.layers):\n",
        "            block = self.block_list1[i]\n",
        "            x = block(x)\n",
        "\n",
        "        #Bridge Block 1\n",
        "        x = self.bridge1_conv1(x)\n",
        "        x = self.bridge1_bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bridge1_conv2(x)\n",
        "        x = self.bridge1_bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        #Basic/Residual Blocks 2\n",
        "        for i in range(self.layers):\n",
        "            block = self.block_list2[i]\n",
        "            x = block(x)\n",
        "\n",
        "        #Bridge Block 2\n",
        "        x = self.bridge2_conv1(x)\n",
        "        x = self.bridge2_bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bridge2_conv2(x)\n",
        "        x = self.bridge2_bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "\n",
        "        #Basic/Residual Blocks 3\n",
        "        for i in range(self.layers):\n",
        "            block = self.block_list3[i]\n",
        "            x = block(x)\n",
        "\n",
        "        #Final pooling and connected layer\n",
        "        x = self.avg_pool(x)\n",
        "\n",
        "        #bs x 64 x 1 x 1 > bs x 64\n",
        "        inputs = x.view(x.size(0), -1)\n",
        "        scores = self.linear_layer(inputs) ##used to be just x input\n",
        "\n",
        "        return scores"
      ],
      "metadata": {
        "id": "ZKErep5SiUWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_on_test_set():\n",
        "    net.eval()\n",
        "    running_error=0\n",
        "    num_batches=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0,10000,bs):\n",
        "\n",
        "            minibatch_data =  test_data[i:i+bs]\n",
        "            minibatch_label= test_label[i:i+bs]\n",
        "\n",
        "            minibatch_data=minibatch_data.to(device)\n",
        "            minibatch_label=minibatch_label.to(device)\n",
        "\n",
        "            inputs = (minibatch_data - mean)/std\n",
        "\n",
        "            scores = net( inputs )\n",
        "\n",
        "            error = utils.get_error( scores , minibatch_label)\n",
        "\n",
        "            running_error += error.item()\n",
        "\n",
        "            num_batches+=1\n",
        "\n",
        "    total_error = running_error/num_batches\n",
        "    print( 'error rate on test set =', total_error*100 ,'percent')"
      ],
      "metadata": {
        "id": "03V13vK_iuF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Network instantiation\n",
        "net = ConvNet(3, 10, 6)\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "fAm094jB_Foy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Network miscellaneous\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "bs = 100"
      ],
      "metadata": {
        "id": "NcgO0od8_Fej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training loop\n",
        "start=time.time()\n",
        "lr = 0.1\n",
        "\n",
        "for epoch in range(50):\n",
        "\n",
        "    if epoch == 40:\n",
        "      lr = lr * .5\n",
        "\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)\n",
        "\n",
        "    running_loss=0\n",
        "    running_error=0\n",
        "    num_batches=0\n",
        "\n",
        "    shuffled_indices=torch.randperm(100000)\n",
        "\n",
        "    for count in range(0,100000,bs):\n",
        "\n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # create a minibatch\n",
        "        indices=shuffled_indices[count:count+bs]\n",
        "        minibatch_data =  train_data[indices]\n",
        "        minibatch_label=  train_label[indices]\n",
        "\n",
        "        # send them to the gpu\n",
        "        minibatch_data=minibatch_data.to(device)\n",
        "        minibatch_label=minibatch_label.to(device)\n",
        "\n",
        "        # subtract the mean and divide by the std\n",
        "        #Do we need this? If the architecture already has a bn implemented?\n",
        "        inputs= (minibatch_data -mean)/std\n",
        "\n",
        "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
        "        inputs.requires_grad_()\n",
        "\n",
        "        # forward the minibatch through the net\n",
        "        scores=net( inputs )\n",
        "        ##scores = net(minibatch_data)\n",
        "\n",
        "        # Compute the average of the losses of the data points in the minibatch\n",
        "        loss =  criterion( scores , minibatch_label)\n",
        "\n",
        "        # backward pass to compute dL/dU, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # START COMPUTING STATS\n",
        "\n",
        "        num_batches+=1\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            error = utils.get_error( scores , minibatch_label)\n",
        "            running_error += error.item()\n",
        "\n",
        "\n",
        "    # compute stats for the full training set\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed = time.time()-start\n",
        "\n",
        "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
        "    eval_on_test_set()\n",
        "    print(' ')"
      ],
      "metadata": {
        "id": "0qWlCArQiuDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test network on random image\n",
        "idx=randint(0, 10000-1)\n",
        "im=test_data[idx]\n",
        "\n",
        "utils.show(im)\n",
        "\n",
        "im = im.to(device)\n",
        "im= (im-mean) / std\n",
        "im=im.view(1,3,32,32)\n",
        "\n",
        "scores =  net(im)\n",
        "probs= F.softmax(scores, dim=1)\n",
        "utils.show_prob_cifar(probs.cpu())"
      ],
      "metadata": {
        "id": "f7NRaAfUiuAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zHuUy0nit7T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}